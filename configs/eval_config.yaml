# RAG Evaluation Configuration

evaluation:
  # Metrics to compute
  metrics:
    - faithfulness
    - answer_relevancy
    - context_precision
    - context_recall

  # Thresholds for passing
  thresholds:
    faithfulness: 0.7
    answer_relevancy: 0.7
    context_precision: 0.6
    context_recall: 0.6
    overall: 0.65

  # Hallucination detection
  hallucination:
    threshold: 0.5
    strict_mode: false

data:
  eval_file: "data/eval_rag.json"
  sample_size: 100  # Number of samples to evaluate

llm:
  # For Ragas evaluation (requires OpenAI API or compatible)
  model: "gpt-3.5-turbo"
  temperature: 0.0

embedding:
  model: "text-embedding-ada-002"

output:
  results_file: "outputs/evaluation_results.json"
  detailed_report: true

mlflow:
  experiment_name: "rag-evaluation"
  tracking_uri: "http://localhost:8080"
  log_artifacts: true
