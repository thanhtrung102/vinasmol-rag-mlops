# VinaSmol Training Configuration
# Optimized for GitHub Codespaces (4 cores, 16GB RAM)

model:
  name: "vinai/PhoGPT-4B-Chat"
  revision: "main"
  trust_remote_code: true

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
  bias: "none"

quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  use_double_quant: true

training:
  num_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-4
  warmup_ratio: 0.03
  max_seq_length: 512
  fp16: true
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"
  max_grad_norm: 0.3

data:
  train_file: "data/train.jsonl"
  eval_file: "data/eval.jsonl"
  text_column: "text"

output:
  dir: "./outputs/vinasmol-lora"
  save_strategy: "epoch"
  save_total_limit: 3
  logging_steps: 10
  resume_from_checkpoint: null

mlflow:
  experiment_name: "vinasmol-lora-training"
  tracking_uri: "http://localhost:8080"
  log_model: true
  run_name: null

# Weights & Biases integration (optional)
wandb:
  enabled: false
  project: "vinasmol-rag-mlops"
  entity: null
  run_name: null
  log_model: "checkpoint"
  tags:
    - vinasmol
    - lora
    - vietnamese

# HuggingFace Hub publishing (optional)
hub:
  enabled: false
  repo_id: null  # e.g., "your-username/vinasmol-lora"
  private: true
  commit_message: "Upload VinaSmol LoRA adapter"
